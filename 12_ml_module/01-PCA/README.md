# 主成分分析(PCA)
- 使用类`cv::PCA`来实现维度分析工作

## 1. 主成分分析原理介绍
顾名思义，主成分分析(PCA)是用来分析数据的主要成分的。

打个比方，一些ｎ维点可以用一个ｎ维的坐标完全表示。主成分分析，可以提取出ｎ个坐标轴来表示这些数据点。不过这ｎ个坐标轴是按顺序求出的，首先根据这些ｎ维点确定第一个坐标轴方向，使得只使用这个坐标轴表示这些二维点误差最小。然后求下一个坐标轴，使得只使用这两个坐标轴表示这些ｎ维点误差最小。依次类推，直到求出n维度坐标轴，便肯定可以完全表示了。

<div style="text-align: center">
<img src="pca/pca_line.png"/>
</div>
---
如上图，是根据一些二维点计算出来的第一个“主成分”。
<div style="text-align: center">
<img src="pca/pca_eigen.png"/>
</div>
---
如上图，是这些二维点的两个**主成分**。


> 由上面分析我们发现，PCA可以用来做**降维**。虽然肯定会有些精度损失，但是这已经是最小的精度损失了。

## 2. 主成分分析(PCA)的代码实现
通常m个n维点排列在一起会得到一个`m*n`矩阵，很奇妙的是，我们可以通过数学推导得出ｎ个特征向量，这个矩阵包含的点集合的**主成分**就是这n个特征向量集合。这些特征向量的代表的主要成分的重要重要程度可以用其对应的特征值来衡量，**特征值越大，其对应的特征向量代表的主成分越重要**。

比如你有$x_1...x_m$总共m个数据，其中每个数据由n个变量表示，我们的目的是将这m个数据使用少于n个变量表示，比如将维度降为`p，p<n`。这个过程叫做**降维**。具体操作步骤如下：
### 2.1 组织数据集
首先将数据整理成如下固定形式。
- 将$x_1...x_m$写成行向量，每一行有n个元素，即n列。
- 将这些行向量放入一个单一的矩阵x，矩阵维度是`m*n`

### 2.2 计算经验均值
- 计算每一个维度的经验均值，维度总共有$j = 1, ..., n$
- 将计算出来的n个经验均值放入一个向量u，这个向量维度是`n*1`

$$ \mathbf{u[j]} = \frac{1}{m}\sum_{i=1}^{m}\mathbf{X[i,j]} $$

### 2.3 计算离均值的偏差矩阵
我们计算均值的原因是我们想把数据搬移到原点附近，让原点处于这些数据的中心，这样可以使得均方误差最小。所以这一步要做的就是：
- 将矩阵x的每一行都减去均值向量u
- 得到一个新的均值矩阵B，维度也是`m*n`

上述操作可以使用如下方式完成

$$ \mathbf{B} = \mathbf{X} - \mathbf{h}\mathbf{u^{T}} $$

其中，h是一个`m*1`的列向量，并且值为全1。

### 2.4 计算上述偏差矩阵的协方差矩阵
- 这一步是计算B矩阵的协方差矩阵C，C矩阵的维度会是`n*n`。可以使用下式实现

$$ \mathbf{C} = \frac{1}{m-1} \mathbf{B^{*}} \cdot \mathbf{B} $$

上式中`*`是共轭转置运算符。不过由于一般B中包含的都是实数，所以在大部分情况下这个操作相当于一个转置运算符。

### 2.5 求得上一步中协方差矩阵的特征值与特征向量
首先说明一下，特征值与特征向量还有矩阵之间的关系。

一般对于一个矩阵C(维度为`n*n`)，会存在n个特征向量(每个维度都是n)，与n个特征值，每个特征值对应一个特征向量。它们之间的关系如下：

$$ \mathbf{V^{-1}} \mathbf{C} \mathbf{V} = \mathbf{D} $$

其中，
- V是n个特征向量组成的`n*n`维矩阵
- D是n个特征值组成的一个对角矩阵（只有对角线上值不为0，维度也是`n*n`）
- 上面V与D中的特征向量与特征值是有序配对的，即第i个特征值对应第i个特征向量

> 到了这一步其实就已经大功告成了，我们下面可以做的就是根据特征值的大小保留前p个特征向量，作为新的维度基础。实现将n维度降维到p维度。

## 3. opencv中的`cv::PCA`类的使用

### 3.1 基础，找出图像块方向
程序分析：
1. 预处理
2. 阈值处理，将图片二值化
3. 找出图像轮廓，轮廓是一些点集合。针对每个轮廓进行主成分分析
4. 主成分分析找出每个轮廓的均值，特征值，特征向量
5. 画出上述，特征向量

### 3.2 人脸图像，主成分分析
程序分析：

